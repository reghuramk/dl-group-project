{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os  # For file path handling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split # Used in Model 4\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Keras / TensorFlow specific imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "# TCN specific import (make sure tcn library is installed: pip install tensorflow-tcn)\n",
    "try:\n",
    "    from tcn import TCN\n",
    "    tcn_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: TCN library not found. Model 2 (TCN) will be skipped.\")\n",
    "    print(\"Install using: pip install tensorflow-tcn\")\n",
    "    tcn_available = False\n",
    "\n",
    "# --- Global Settings ---\n",
    "SEED = 44\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED) # Also set numpy seed for consistency\n",
    "random.seed(SEED)\n",
    "\n",
    "FILE_PATH = \"starbucks_open_7year - starbucks_open_7year.csv.csv\" # Adjust if needed\n",
    "SEQ_LENGTH = 60  # Sequence length for time series models (Models 1, 2, 4)\n",
    "FUTURE_DAYS_TO_PREDICT = 30 # How many days into the future to predict\n",
    "\n",
    "\n",
    "# --- Data Loading and Initial Preparation ---\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"Loads, cleans, and prepares the initial Starbucks stock data.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: Data file not found at {file_path}\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"--- Initial Data Info ---\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nMissing values before fill:\\n{df.isna().sum()}\")\n",
    "\n",
    "    # Handle potential missing values (using forward fill as in original scripts)\n",
    "    df.ffill(inplace=True)\n",
    "    print(f\"\\nMissing values after ffill:\\n{df.isna().sum()}\")\n",
    "\n",
    "    # Convert Date and set as index\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors='coerce') # Added errors='coerce' for robustness\n",
    "    df.dropna(subset=['Date'], inplace=True) # Drop rows where date conversion failed\n",
    "    df = df.set_index(\"Date\").sort_index()\n",
    "\n",
    "    # Log transform (common for stock prices)\n",
    "    # Use a small epsilon to avoid log(0) if 'Open' could be 0\n",
    "    epsilon = 1e-9\n",
    "    df[\"Log_Open\"] = np.log(df[\"Open\"] + epsilon)\n",
    "\n",
    "    # Add smoothed version (used in Model 4)\n",
    "    df['Smoothed_Log_Open'] = df['Log_Open'].rolling(window=3).mean().fillna(df['Log_Open'])\n",
    "\n",
    "    print(\"\\n--- Data After Initial Prep ---\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nData shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# --- Data Preparation Utilities ---\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"Creates sequences for time series forecasting models.\"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - seq_length): # Adjusted range to prevent index out of bounds\n",
    "        X.append(data[i:(i + seq_length), 0])\n",
    "        Y.append(data[i + seq_length, 0])\n",
    "    if not X: # Handle cases where data is too short for sequence length\n",
    "        return np.array([]), np.array([])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def create_lag_features(data, window):\n",
    "    \"\"\"Creates lag features (alternative sequence creation for Model 4).\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(data)):\n",
    "        X.append(data[i - window:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- Model Definitions ---\n",
    "\n",
    "# Model 1: LSTM\n",
    "def build_lstm_model(seq_length):\n",
    "    \"\"\"Builds the LSTM model architecture.\"\"\"\n",
    "    model = Sequential(name=\"LSTM_Model\")\n",
    "    # Using tanh activation (common for LSTM) and specifying recurrent_activation\n",
    "    model.add(LSTM(100, activation=\"tanh\", recurrent_activation='sigmoid', return_sequences=True,\n",
    "                   recurrent_dropout=0.2, input_shape=(seq_length, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(80, activation=\"tanh\", recurrent_activation='sigmoid', recurrent_dropout=0.3)) # Removed return_sequences=False (default)\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(50, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(30, activation=\"relu\"))\n",
    "    model.add(Dense(15, activation=\"relu\"))\n",
    "    model.add(Dense(1)) # Linear activation for regression output\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    return model\n",
    "\n",
    "# Model 2: TCN\n",
    "def build_tcn_model(seq_length):\n",
    "    \"\"\"Builds the TCN model architecture.\"\"\"\n",
    "    if not tcn_available:\n",
    "        return None\n",
    "    model = Sequential(name=\"TCN_Model\")\n",
    "    model.add(TCN(input_shape=(seq_length, 1),\n",
    "                  nb_filters=128,\n",
    "                  kernel_size=3,\n",
    "                  nb_stacks=1,\n",
    "                  dilations=[1, 2, 4, 8, 16],\n",
    "                  padding='causal', # Causal padding is important for time series\n",
    "                  use_skip_connections=True,\n",
    "                  dropout_rate=0.1, # Original dropout was after TCN layer\n",
    "                  activation='relu')) # Specify activation within TCN\n",
    "    # model.add(Dropout(0.1)) # Dropout is often applied within TCN layer now\n",
    "    model.add(Dense(1)) # Linear activation for regression output\n",
    "    # Using Adam optimizer and Huber loss as in the original script\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Model 3: Polynomial Regression DNN\n",
    "def build_poly_dnn_model(input_shape):\n",
    "    \"\"\"Builds the DNN model for polynomial features.\"\"\"\n",
    "    model = Sequential(name=\"Polynomial_DNN_Model\")\n",
    "    model.add(Dense(512, activation='relu', input_dim=input_shape)) # Correct input_dim usage\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128)) # Consider adding activation here if needed, e.g., 'relu'\n",
    "    model.add(LeakyReLU(alpha=0.1)) # Applied after the Dense layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1)) # Linear activation for regression output\n",
    "    model.compile(optimizer='adam', loss='mse') # Using Adam and MSE as in original\n",
    "    return model\n",
    "\n",
    "# Model 4: Lag Feature DNN\n",
    "def build_lag_dnn_model(input_shape):\n",
    "    \"\"\"Builds the DNN model for lagged features.\"\"\"\n",
    "    model = Sequential(name=\"Lag_Feature_DNN_Model\")\n",
    "    model.add(Dense(256, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1)) # Linear activation for regression output\n",
    "    optimizer = Adam(learning_rate=0.0005) # Using Adam optimizer as in original\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# --- Training and Evaluation ---\n",
    "def evaluate_model(y_true, y_pred, model_name, data_type=\"Test\"):\n",
    "    \"\"\"Calculates and prints evaluation metrics.\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n--- {model_name} - {data_type} Set Evaluation ---\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"MSE:  {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2:   {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}\")\n",
    "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape}\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plots the training and validation loss.\"\"\"\n",
    "    if history and 'loss' in history.history and 'val_loss' in history.history:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} - Training History')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No training history found or history object incomplete for {model_name}.\")\n",
    "\n",
    "\n",
    "def plot_predictions(dates, y_actual, y_pred, model_name, title_suffix=\"Prediction\"):\n",
    "    \"\"\"Plots actual vs. predicted values.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dates, y_actual, label='Actual Price', color='blue', marker='.', linestyle='-')\n",
    "    plt.plot(dates, y_pred, label='Predicted Price', color='red', marker='.', linestyle='--')\n",
    "    plt.title(f'Starbucks Stock Price - {model_name} {title_suffix}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Stock Price (Log or Actual)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Future Prediction Function ---\n",
    "def predict_future(model, last_sequence, seq_length, future_days, scaler):\n",
    "    \"\"\"Predicts future stock prices iteratively.\"\"\"\n",
    "    predictions_scaled = []\n",
    "    current_sequence = last_sequence.copy().reshape(1, seq_length, 1) # Reshape for model input\n",
    "\n",
    "    for _ in range(future_days):\n",
    "        # Predict the next step\n",
    "        next_pred_scaled = model.predict(current_sequence, verbose=0)[0, 0]\n",
    "        predictions_scaled.append(next_pred_scaled)\n",
    "\n",
    "        # Prepare the next input sequence: roll and append prediction\n",
    "        # Create the new sequence for the next prediction step\n",
    "        # Remove the first element and append the prediction at the end\n",
    "        next_sequence_values = np.append(current_sequence[0, 1:, 0], next_pred_scaled)\n",
    "        current_sequence = next_sequence_values.reshape(1, seq_length, 1)\n",
    "\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    future_predictions = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "    return future_predictions\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        sb_df = load_and_prepare_data(FILE_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        exit() # Exit if data file is not found\n",
    "\n",
    "    # --- Preparation for Time Series Models (1, 2, 4) ---\n",
    "    print(\"\\n--- Preparing Data for Time Series Models (LSTM, TCN, Lag DNN) ---\")\n",
    "    # Using Log_Open for Models 1 & 2, Smoothed_Log_Open for Model 4\n",
    "    data_log_open = sb_df[[\"Log_Open\"]].values\n",
    "    data_smoothed_log_open = sb_df[[\"Smoothed_Log_Open\"]].values\n",
    "\n",
    "    # Scale data\n",
    "    scaler_log = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_log_data = scaler_log.fit_transform(data_log_open)\n",
    "\n",
    "    scaler_smoothed = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_smoothed_data = scaler_smoothed.fit_transform(data_smoothed_log_open)\n",
    "\n",
    "    # Create sequences for LSTM/TCN (using Log_Open)\n",
    "    X_seq, Y_seq = create_sequences(scaled_log_data, SEQ_LENGTH)\n",
    "\n",
    "    # Create lag features for Lag DNN (using Smoothed_Log_Open)\n",
    "    X_lag, y_lag = create_lag_features(scaled_smoothed_data, window=SEQ_LENGTH) # window = SEQ_LENGTH for consistency\n",
    "\n",
    "    if X_seq.size == 0 or X_lag.size == 0:\n",
    "        print(\"Error: Not enough data to create sequences/lags with the specified length.\")\n",
    "        exit()\n",
    "\n",
    "    # Split data (TimeSeries Split - chronological)\n",
    "    # Using common split ratios from Model 1 (70/15/15) for consistency\n",
    "    train_size_idx = int(len(X_seq) * 0.7)\n",
    "    val_size_idx = int(len(X_seq) * (0.7 + 0.15))\n",
    "\n",
    "    X_train_seq, Y_train_seq = X_seq[:train_size_idx], Y_seq[:train_size_idx]\n",
    "    X_val_seq, Y_val_seq = X_seq[train_size_idx:val_size_idx], Y_seq[train_size_idx:val_size_idx]\n",
    "    X_test_seq, Y_test_seq = X_seq[val_size_idx:], Y_seq[val_size_idx:]\n",
    "\n",
    "    # Split for Lag DNN (using same indices for comparable test set size)\n",
    "    X_train_lag, y_train_lag = X_lag[:train_size_idx], y_lag[:train_size_idx]\n",
    "    X_val_lag, y_val_lag = X_lag[train_size_idx:val_size_idx], y_lag[train_size_idx:val_size_idx]\n",
    "    X_test_lag, y_test_lag = X_lag[val_size_idx:], y_lag[val_size_idx:]\n",
    "\n",
    "\n",
    "    # Reshape/Flatten inputs\n",
    "    X_train_lstm_tcn = np.reshape(X_train_seq, (X_train_seq.shape[0], X_train_seq.shape[1], 1))\n",
    "    X_val_lstm_tcn = np.reshape(X_val_seq, (X_val_seq.shape[0], X_val_seq.shape[1], 1))\n",
    "    X_test_lstm_tcn = np.reshape(X_test_seq, (X_test_seq.shape[0], X_test_seq.shape[1], 1))\n",
    "\n",
    "    X_train_lag_flat = X_train_lag.reshape(X_train_lag.shape[0], -1)\n",
    "    X_val_lag_flat = X_val_lag.reshape(X_val_lag.shape[0], -1)\n",
    "    X_test_lag_flat = X_test_lag.reshape(X_test_lag.shape[0], -1)\n",
    "\n",
    "    print(f\"LSTM/TCN Train shape: X-{X_train_lstm_tcn.shape}, Y-{Y_train_seq.shape}\")\n",
    "    print(f\"LSTM/TCN Val shape:   X-{X_val_lstm_tcn.shape}, Y-{Y_val_seq.shape}\")\n",
    "    print(f\"LSTM/TCN Test shape:  X-{X_test_lstm_tcn.shape}, Y-{Y_test_seq.shape}\")\n",
    "    print(f\"Lag DNN Train shape: X-{X_train_lag_flat.shape}, Y-{y_train_lag.shape}\")\n",
    "    print(f\"Lag DNN Val shape:   X-{X_val_lag_flat.shape}, Y-{y_val_lag.shape}\")\n",
    "    print(f\"Lag DNN Test shape:  X-{X_test_lag_flat.shape}, Y-{y_test_lag.shape}\")\n",
    "\n",
    "    # --- Preparation for Polynomial DNN Model (Model 3) ---\n",
    "    print(\"\\n--- Preparing Data for Polynomial DNN Model ---\")\n",
    "    df_poly = sb_df.copy() # Use a copy to avoid modifying the original df\n",
    "\n",
    "    # Feature Engineering (Days, Days^2, Days^3) - applied to the original 'Open' price\n",
    "    df_poly['Days'] = (df_poly.index - df_poly.index.min()).days\n",
    "    df_poly['Days2'] = df_poly['Days'] ** 2\n",
    "    df_poly['Days3'] = df_poly['Days'] ** 3\n",
    "\n",
    "    X_poly_feat = df_poly[['Days', 'Days2', 'Days3']].values.astype(float)\n",
    "    y_poly_raw = df_poly['Open'].values.astype(float).reshape(-1, 1) # Target is raw 'Open'\n",
    "\n",
    "    # Scale features and target separately for this model\n",
    "    scaler_X_poly = MinMaxScaler()\n",
    "    X_poly_scaled = scaler_X_poly.fit_transform(X_poly_feat)\n",
    "\n",
    "    scaler_y_poly = MinMaxScaler()\n",
    "    y_poly_scaled = scaler_y_poly.fit_transform(y_poly_raw)\n",
    "\n",
    "    # Split data (using standard train_test_split, 80/20 split as per original)\n",
    "    # Note: This split is NOT chronological, unlike the time series models\n",
    "    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
    "        X_poly_scaled, y_poly_scaled, test_size=0.20, random_state=SEED, shuffle=False # Maintain order if possible\n",
    "    )\n",
    "    # Further split training into train/validation (e.g., 80% of original train -> 64% total)\n",
    "    X_train_poly, X_val_poly, y_train_poly, y_val_poly = train_test_split(\n",
    "         X_train_poly, y_train_poly, test_size=0.1875, random_state=SEED, shuffle=False # 0.15 / 0.8 = 0.1875\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Poly DNN Train shape: X-{X_train_poly.shape}, Y-{y_train_poly.shape}\")\n",
    "    print(f\"Poly DNN Val shape:   X-{X_val_poly.shape}, Y-{y_val_poly.shape}\")\n",
    "    print(f\"Poly DNN Test shape:  X-{X_test_poly.shape}, Y-{y_test_poly.shape}\")\n",
    "\n",
    "    # --- Define Callbacks ---\n",
    "    early_stop_mse = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1) # Increased patience slightly\n",
    "    reduce_lr_mse = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    early_stop_huber = EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True, verbose=1) # Slightly different patience for Huber loss model (TCN)\n",
    "    reduce_lr_huber = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "\n",
    "    # --- Model Training and Evaluation ---\n",
    "    models_results = {}\n",
    "\n",
    "    # === Model 1: LSTM ===\n",
    "    print(\"\\n=== Training Model 1: LSTM ===\")\n",
    "    model_lstm = build_lstm_model(SEQ_LENGTH)\n",
    "    model_lstm.summary()\n",
    "    history_lstm = model_lstm.fit(X_train_lstm_tcn, Y_train_seq, epochs=100,\n",
    "                                  batch_size=32, validation_data=(X_val_lstm_tcn, Y_val_seq),\n",
    "                                  callbacks=[early_stop_mse, reduce_lr_mse], verbose=1)\n",
    "    plot_training_history(history_lstm, \"LSTM\")\n",
    "\n",
    "    # Evaluate LSTM\n",
    "    Y_pred_lstm_scaled = model_lstm.predict(X_test_lstm_tcn)\n",
    "    evaluate_model(Y_test_seq, Y_pred_lstm_scaled, \"LSTM\", \"Test (Scaled)\")\n",
    "    Y_test_lstm_actual = scaler_log.inverse_transform(Y_test_seq.reshape(-1, 1))\n",
    "    Y_pred_lstm_actual = scaler_log.inverse_transform(Y_pred_lstm_scaled)\n",
    "    lstm_metrics = evaluate_model(Y_test_lstm_actual, Y_pred_lstm_actual, \"LSTM\", \"Test (Actual Price)\")\n",
    "    models_results[\"LSTM\"] = lstm_metrics\n",
    "\n",
    "    # Plot LSTM Predictions\n",
    "    test_dates_lstm = sb_df.index[val_size_idx + SEQ_LENGTH:] # Adjust index slicing\n",
    "    if len(test_dates_lstm) == len(Y_test_lstm_actual):\n",
    "         plot_predictions(test_dates_lstm, Y_test_lstm_actual, Y_pred_lstm_actual, \"LSTM\", \"Actual vs Predicted (Log_Open)\")\n",
    "    else:\n",
    "        print(f\"Warning: Date length mismatch for LSTM plot ({len(test_dates_lstm)} vs {len(Y_test_lstm_actual)})\")\n",
    "\n",
    "\n",
    "    # === Model 2: TCN ===\n",
    "    if tcn_available:\n",
    "        print(\"\\n=== Training Model 2: TCN ===\")\n",
    "        model_tcn = build_tcn_model(SEQ_LENGTH)\n",
    "        if model_tcn:\n",
    "            model_tcn.summary()\n",
    "            history_tcn = model_tcn.fit(X_train_lstm_tcn, Y_train_seq, epochs=100,\n",
    "                                      batch_size=32, validation_data=(X_val_lstm_tcn, Y_val_seq),\n",
    "                                      callbacks=[early_stop_huber, reduce_lr_huber], verbose=1) # Using huber callbacks\n",
    "            plot_training_history(history_tcn, \"TCN\")\n",
    "\n",
    "            # Evaluate TCN\n",
    "            Y_pred_tcn_scaled = model_tcn.predict(X_test_lstm_tcn)\n",
    "            evaluate_model(Y_test_seq, Y_pred_tcn_scaled, \"TCN\", \"Test (Scaled)\")\n",
    "            Y_test_tcn_actual = scaler_log.inverse_transform(Y_test_seq.reshape(-1, 1)) # Uses same scaler as LSTM\n",
    "            Y_pred_tcn_actual = scaler_log.inverse_transform(Y_pred_tcn_scaled)\n",
    "            tcn_metrics = evaluate_model(Y_test_tcn_actual, Y_pred_tcn_actual, \"TCN\", \"Test (Actual Price)\")\n",
    "            models_results[\"TCN\"] = tcn_metrics\n",
    "\n",
    "            # Plot TCN Predictions (using same dates as LSTM test set)\n",
    "            if len(test_dates_lstm) == len(Y_test_tcn_actual):\n",
    "                plot_predictions(test_dates_lstm, Y_test_tcn_actual, Y_pred_tcn_actual, \"TCN\", \"Actual vs Predicted (Log_Open)\")\n",
    "            else:\n",
    "                 print(f\"Warning: Date length mismatch for TCN plot ({len(test_dates_lstm)} vs {len(Y_test_tcn_actual)})\")\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping TCN model building.\")\n",
    "    else:\n",
    "         print(\"\\nSkipping Model 2: TCN (library not installed)\")\n",
    "\n",
    "\n",
    "    # === Model 3: Polynomial DNN ===\n",
    "    print(\"\\n=== Training Model 3: Polynomial DNN ===\")\n",
    "    model_poly_dnn = build_poly_dnn_model(X_train_poly.shape[1])\n",
    "    model_poly_dnn.summary()\n",
    "    history_poly = model_poly_dnn.fit(X_train_poly, y_train_poly, epochs=500, # Original had 500 epochs\n",
    "                                      batch_size=16, validation_data=(X_val_poly, y_val_poly),\n",
    "                                      callbacks=[early_stop_mse, reduce_lr_mse], verbose=1) # Using MSE callbacks\n",
    "    plot_training_history(history_poly, \"Polynomial DNN\")\n",
    "\n",
    "    # Evaluate Polynomial DNN\n",
    "    y_pred_poly_scaled = model_poly_dnn.predict(X_test_poly)\n",
    "    evaluate_model(y_test_poly, y_pred_poly_scaled, \"Polynomial DNN\", \"Test (Scaled)\")\n",
    "    y_test_poly_actual = scaler_y_poly.inverse_transform(y_test_poly)\n",
    "    y_pred_poly_actual = scaler_y_poly.inverse_transform(y_pred_poly_scaled)\n",
    "    poly_dnn_metrics = evaluate_model(y_test_poly_actual, y_pred_poly_actual, \"Polynomial DNN\", \"Test (Actual Price)\")\n",
    "    models_results[\"Polynomial DNN\"] = poly_dnn_metrics\n",
    "\n",
    "    # Plot Polynomial DNN Predictions\n",
    "    # Need to reconstruct the dates for the test set used here\n",
    "    test_indices_poly = sb_df.iloc[len(X_train_poly)+len(X_val_poly):].index # Indices corresponding to test set\n",
    "    test_dates_poly = df_poly.loc[test_indices_poly].index # Get dates using original poly df index\n",
    "    if len(test_dates_poly) == len(y_test_poly_actual):\n",
    "         plot_predictions(test_dates_poly, y_test_poly_actual, y_pred_poly_actual, \"Polynomial DNN\", \"Actual vs Predicted (Raw Open)\")\n",
    "    else:\n",
    "        print(f\"Warning: Date length mismatch for Poly DNN plot ({len(test_dates_poly)} vs {len(y_test_poly_actual)})\")\n",
    "\n",
    "\n",
    "    # === Model 4: Lag Feature DNN ===\n",
    "    print(\"\\n=== Training Model 4: Lag Feature DNN ===\")\n",
    "    model_lag_dnn = build_lag_dnn_model(X_train_lag_flat.shape[1])\n",
    "    model_lag_dnn.summary()\n",
    "    history_lag = model_lag_dnn.fit(X_train_lag_flat, y_train_lag, epochs=200, # Original had 200 epochs\n",
    "                                     batch_size=32, validation_data=(X_val_lag_flat, y_val_lag),\n",
    "                                     callbacks=[early_stop_mse, reduce_lr_mse], verbose=1) # Using MSE callbacks\n",
    "    plot_training_history(history_lag, \"Lag Feature DNN\")\n",
    "\n",
    "    # Evaluate Lag Feature DNN\n",
    "    y_pred_lag_scaled = model_lag_dnn.predict(X_test_lag_flat)\n",
    "    evaluate_model(y_test_lag, y_pred_lag_scaled, \"Lag Feature DNN\", \"Test (Scaled - Smoothed Log)\")\n",
    "    y_test_lag_actual = scaler_smoothed.inverse_transform(y_test_lag) # Use the smoothed scaler\n",
    "    y_pred_lag_actual = scaler_smoothed.inverse_transform(y_pred_lag_scaled)\n",
    "    lag_dnn_metrics = evaluate_model(y_test_lag_actual, y_pred_lag_actual, \"Lag Feature DNN\", \"Test (Actual Price - based on Smoothed Log)\")\n",
    "    models_results[\"Lag Feature DNN\"] = lag_dnn_metrics\n",
    "\n",
    "    # Plot Lag Feature DNN Predictions (using same dates as LSTM/TCN test set)\n",
    "    if len(test_dates_lstm) == len(y_test_lag_actual): # Note: Comparing Log_Open actuals to Smoothed_Log_Open predictions\n",
    "        plot_predictions(test_dates_lstm, Y_test_lstm_actual, y_pred_lag_actual, \"Lag Feature DNN\", \"Actual (Log Open) vs Predicted (Smoothed Log Open)\")\n",
    "    else:\n",
    "        print(f\"Warning: Date length mismatch for Lag DNN plot ({len(test_dates_lstm)} vs {len(y_test_lag_actual)})\")\n",
    "\n",
    "\n",
    "    # --- Compare Model Results ---\n",
    "    print(\"\\n--- Final Model Comparison (Actual Price Metrics on Test Set) ---\")\n",
    "    results_df = pd.DataFrame(models_results).T # Transpose for better readability\n",
    "    print(results_df.sort_values(by='MAPE')) # Sort by MAPE, for example\n",
    "\n",
    "\n",
    "    # --- Future Predictions (Example using LSTM model) ---\n",
    "    print(f\"\\n--- Predicting Next {FUTURE_DAYS_TO_PREDICT} Days using LSTM Model ---\")\n",
    "    last_sequence_scaled = scaled_log_data[-SEQ_LENGTH:] # Get the last sequence from the scaled log data\n",
    "    future_preds_log = predict_future(model_lstm, last_sequence_scaled, SEQ_LENGTH, FUTURE_DAYS_TO_PREDICT, scaler_log)\n",
    "\n",
    "    # Create future dates\n",
    "    last_date = sb_df.index[-1]\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=FUTURE_DAYS_TO_PREDICT, freq=\"D\") # Assuming daily predictions\n",
    "\n",
    "    # Plot Future Predictions\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    # Plotting historical actual 'Open' price for context\n",
    "    plt.plot(sb_df.index, scaler_log.inverse_transform(data_log_open) , label=\"Historical Actual (Log Open)\", color='blue')\n",
    "    # Plotting future predictions\n",
    "    plt.plot(future_dates, future_preds_log, \"r-\", label=f\"Future {FUTURE_DAYS_TO_PREDICT}-Day Predictions (LSTM)\", marker='o')\n",
    "    plt.title(\"Starbucks Stock Price: Historical Data and Future Predictions (LSTM)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Stock Price (Log Open)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7828cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
